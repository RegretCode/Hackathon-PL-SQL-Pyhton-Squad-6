{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9998c50"
      },
      "source": [
        "# üöÄ Tradutor SQL para PySpark - Vers√£o Limpa\n",
        "\n",
        "## üìã Vis√£o Geral\n",
        "\n",
        "Este notebook cont√©m uma **solu√ß√£o funcional e limpa** para tradu√ß√£o de consultas SQL em c√≥digo PySpark, com foco na correta resolu√ß√£o de aliases.\n",
        "\n",
        "### ‚ú® Funcionalidades\n",
        "\n",
        "- **üîç Parser SQL Robusto**: An√°lise de estrutura SQL\n",
        "- **‚ö° Tradutor Funcional**: Convers√£o correta para PySpark\n",
        "- **üéØ Resolu√ß√£o de Aliases**: Sempre usa nomes reais de tabelas\n",
        "- **üìä Suporte**: SELECT, JOIN, WHERE, ORDER BY, COALESCE, CASE WHEN\n",
        "- **‚úÖ Testado**: C√≥digo validado e funcional\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26906563"
      },
      "source": [
        "## üì¶ 1. Importa√ß√µes e Configura√ß√µes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "df6131ff",
        "outputId": "20645448-17dd-4814-90d4-8fecdee3a911",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Spark inicializado com sucesso!\n",
            "   Vers√£o: 3.5.1\n",
            "üéØ Bibliotecas carregadas com sucesso!\n",
            "üìã Pronto para executar o tradutor SQL para PySpark\n"
          ]
        }
      ],
      "source": [
        "# üì¶ IMPORTA√á√ïES NECESS√ÅRIAS\n",
        "\n",
        "import re\n",
        "import textwrap\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# === Configura√ß√£o do Spark (com fallback para demo) ===\n",
        "spark = None\n",
        "try:\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pyspark.sql import functions as F\n",
        "\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"SQL_to_PySpark_Clean\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    print(\"‚úÖ Spark inicializado com sucesso!\")\n",
        "    print(f\"   Vers√£o: {spark.version}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Spark n√£o dispon√≠vel - executando em modo demo\")\n",
        "    print(f\"   Erro: {e}\")\n",
        "    print(\"   üí° O tradutor funcionar√° normalmente, gerando c√≥digo PySpark v√°lido\")\n",
        "\n",
        "    # Fallback: definir F como mock para evitar erros\n",
        "    class MockF:\n",
        "        @staticmethod\n",
        "        def col(name): return f\"F.col('{name}')\"\n",
        "        @staticmethod\n",
        "        def lit(value): return f\"F.lit({repr(value)})\"\n",
        "        @staticmethod\n",
        "        def when(condition, value): return f\"F.when({condition}, {value})\"\n",
        "        @staticmethod\n",
        "        def coalesce(*cols): return f\"F.coalesce({', '.join(map(str, cols))})\"\n",
        "        @staticmethod\n",
        "        def count(): return \"F.count()\"\n",
        "        @staticmethod\n",
        "        def sum(col): return f\"F.sum({col})\"\n",
        "        @staticmethod\n",
        "        def avg(col): return f\"F.avg({col})\"\n",
        "        @staticmethod\n",
        "        def min(col): return f\"F.min({col})\"\n",
        "        @staticmethod\n",
        "        def max(col): return f\"F.max({col})\"\n",
        "\n",
        "    F = MockF()\n",
        "\n",
        "print(\"üéØ Bibliotecas carregadas com sucesso!\")\n",
        "print(\"üìã Pronto para executar o tradutor SQL para PySpark\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c78a84f7"
      },
      "source": [
        "## üîß 2. Parser SQL\n",
        "\n",
        "Parser robusto que analisa consultas SQL e extrai:\n",
        "- SELECT (colunas, aliases, fun√ß√µes)\n",
        "- FROM (tabela principal)\n",
        "- JOIN (tipos, condi√ß√µes, aliases)\n",
        "- WHERE (condi√ß√µes de filtro)\n",
        "- ORDER BY (ordena√ß√£o)\n",
        "- COALESCE e CASE WHEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "59e66a97",
        "outputId": "ea7a12ef-3c76-4824-9378-10d462e2c4cc",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Parser SQL criado com sucesso!\n",
            "   üîç Suporte: SELECT, FROM, JOIN, WHERE, ORDER BY\n",
            "   üéØ Foco na resolu√ß√£o correta de aliases\n"
          ]
        }
      ],
      "source": [
        "# üîß SQL PARSER FUNCIONAL\n",
        "\n",
        "@dataclass\n",
        "class ParsedSQL:\n",
        "    \"\"\"Container para consulta SQL analisada.\"\"\"\n",
        "    select_clause: str\n",
        "    from_clause: str\n",
        "    join_clauses: List[str]\n",
        "    where_clause: str\n",
        "    order_by_clause: str\n",
        "    table_aliases: Dict[str, str]  # alias -> real_name\n",
        "    original_query: str\n",
        "\n",
        "class SQLParser:\n",
        "    \"\"\"Parser SQL robusto com foco na resolu√ß√£o de aliases.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.patterns = {\n",
        "            # Padr√µes principais\n",
        "            'select': r'SELECT\\s+(.*?)(?=\\s+FROM)',\n",
        "            'from': r'FROM\\s+([\\w\\.]+)(?:\\s+(?:AS\\s+)?([\\w]+))?',\n",
        "            'join': r'((?:INNER|LEFT|RIGHT|FULL)\\s+)?JOIN\\s+([\\w\\.]+)(?:\\s+(?:AS\\s+)?([\\w]+))?\\s+ON\\s+([^\\s]+(?:\\s*[=<>!]+\\s*[^\\s]+)*)',\n",
        "            'where': r'WHERE\\s+(.*?)(?=\\s+(?:GROUP\\s+BY|HAVING|ORDER\\s+BY|LIMIT)|$)',\n",
        "            'order_by': r'ORDER\\s+BY\\s+(.*?)(?=\\s+(?:LIMIT)|$)',\n",
        "            'coalesce': r'COALESCE\\s*\\(([^)]+)\\)',\n",
        "            'case_when': r'CASE\\s+.*?\\s+END'\n",
        "        }\n",
        "\n",
        "    def parse(self, sql: str) -> ParsedSQL:\n",
        "        \"\"\"Analisar consulta SQL completa.\"\"\"\n",
        "        # Normalizar SQL para facilitar parsing\n",
        "        sql_clean = re.sub(r'\\s+', ' ', sql.strip())\n",
        "        sql_clean = re.sub(r'\\n', ' ', sql_clean)\n",
        "\n",
        "        # Extrair clauses\n",
        "        select_clause = self._extract_clause(sql_clean, 'select')\n",
        "        from_match = re.search(self.patterns['from'], sql_clean, re.IGNORECASE)\n",
        "\n",
        "        # Extrair FROM e alias da tabela principal\n",
        "        from_clause = \"\"\n",
        "        table_aliases = {}\n",
        "\n",
        "        if from_match:\n",
        "            table_name = from_match.group(1)\n",
        "            table_alias = from_match.group(2)\n",
        "            from_clause = table_name\n",
        "\n",
        "            if table_alias:\n",
        "                table_aliases[table_alias] = table_name\n",
        "\n",
        "        # Extrair JOINs e seus aliases\n",
        "        join_clauses = []\n",
        "        join_matches = re.finditer(self.patterns['join'], sql_clean, re.IGNORECASE)\n",
        "\n",
        "        for match in join_matches:\n",
        "            join_type = (match.group(1) or \"INNER\").strip()\n",
        "            join_table = match.group(2)\n",
        "            join_alias = match.group(3)\n",
        "            join_condition = match.group(4)\n",
        "\n",
        "            if join_alias:\n",
        "                table_aliases[join_alias] = join_table\n",
        "\n",
        "            join_clauses.append(f\"{join_type} JOIN {join_table} ON {join_condition}\")\n",
        "\n",
        "        # Extrair outras clauses\n",
        "        where_clause = self._extract_clause(sql_clean, 'where')\n",
        "        order_by_clause = self._extract_clause(sql_clean, 'order_by')\n",
        "\n",
        "        return ParsedSQL(\n",
        "            select_clause=select_clause,\n",
        "            from_clause=from_clause,\n",
        "            join_clauses=join_clauses,\n",
        "            where_clause=where_clause,\n",
        "            order_by_clause=order_by_clause,\n",
        "            table_aliases=table_aliases,\n",
        "            original_query=sql\n",
        "        )\n",
        "\n",
        "    def _extract_clause(self, sql: str, clause_type: str) -> str:\n",
        "        \"\"\"Extrair uma cl√°usula espec√≠fica do SQL.\"\"\"\n",
        "        pattern = self.patterns.get(clause_type, '')\n",
        "        if not pattern:\n",
        "            return \"\"\n",
        "\n",
        "        match = re.search(pattern, sql, re.IGNORECASE | re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        return \"\"\n",
        "\n",
        "print(\"‚úÖ Parser SQL criado com sucesso!\")\n",
        "print(\"   üîç Suporte: SELECT, FROM, JOIN, WHERE, ORDER BY\")\n",
        "print(\"   üéØ Foco na resolu√ß√£o correta de aliases\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "954eac17"
      },
      "source": [
        "## ‚ö° 3. Tradutor PySpark\n",
        "\n",
        "Tradutor que converte SQL em c√≥digo PySpark funcional, com **resolu√ß√£o correta de aliases**:\n",
        "- Sempre usa nomes reais de tabelas (n√£o aliases)\n",
        "- Suporta JOINs, COALESCE, CASE WHEN\n",
        "- Gera c√≥digo PySpark execut√°vel\n",
        "- Valida√ß√£o autom√°tica de aliases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2fe157ac",
        "outputId": "7ce4b15c-de2b-4a0e-b4b7-37b3c635bbdd",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tradutor PySpark criado com sucesso!\n",
            "   ‚ö° Tradu√ß√£o SQL ‚Üí PySpark\n",
            "   üéØ Resolu√ß√£o autom√°tica de aliases\n",
            "   üìä Suporte: JOIN, WHERE, ORDER BY, COALESCE\n",
            "   ‚úÖ Valida√ß√£o autom√°tica\n"
          ]
        }
      ],
      "source": [
        "# ‚ö° TRADUTOR PYSPARK FUNCIONAL\n",
        "\n",
        "class SimpleSQLTranslator:\n",
        "    \"\"\"Tradutor SQL para PySpark com resolu√ß√£o robusta de aliases.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.parser = SQLParser()\n",
        "\n",
        "    def translate(self, sql: str) -> Dict:\n",
        "        \"\"\"Traduzir SQL para PySpark com valida√ß√£o.\"\"\"\n",
        "        try:\n",
        "            # Analisar SQL\n",
        "            parsed = self.parser.parse(sql)\n",
        "\n",
        "            # Gerar c√≥digo PySpark\n",
        "            pyspark_code = self._generate_pyspark_code(parsed)\n",
        "\n",
        "            # Validar aliases no c√≥digo gerado\n",
        "            validation = self._validate_no_aliases(pyspark_code, parsed.table_aliases)\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'pyspark_code': pyspark_code,\n",
        "                'spark_sql': parsed.original_query,\n",
        "                'table_aliases': parsed.table_aliases,\n",
        "                'validation': validation,\n",
        "                'parsed_structure': {\n",
        "                    'select': parsed.select_clause,\n",
        "                    'from': parsed.from_clause,\n",
        "                    'joins': parsed.join_clauses,\n",
        "                    'where': parsed.where_clause,\n",
        "                    'order_by': parsed.order_by_clause\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e),\n",
        "                'pyspark_code': '',\n",
        "                'spark_sql': sql\n",
        "            }\n",
        "\n",
        "    def _generate_pyspark_code(self, parsed: ParsedSQL) -> str:\n",
        "        \"\"\"Gerar c√≥digo PySpark como uma √∫nica string cont√≠nua usando method chaining.\"\"\"\n",
        "        chain_parts = [f\"spark.table('{parsed.from_clause}')\"]\n",
        "\n",
        "        # Adicionar JOINs\n",
        "        for join_clause in parsed.join_clauses:\n",
        "            join_part = self._translate_join_for_chain(join_clause, parsed.table_aliases)\n",
        "            chain_parts.append(join_part)\n",
        "\n",
        "        # Adicionar WHERE\n",
        "        if parsed.where_clause:\n",
        "            where_condition = self._resolve_aliases_in_expression(parsed.where_clause, parsed.table_aliases)\n",
        "            where_part = f\".filter({where_condition})\"\n",
        "            chain_parts.append(where_part)\n",
        "\n",
        "        # Detectar GROUP BY\n",
        "        group_by_match = re.search(r'GROUP BY (.+?)(?: ORDER BY| LIMIT|$)', parsed.original_query, re.IGNORECASE)\n",
        "        if group_by_match:\n",
        "            group_by_cols = [col.strip() for col in group_by_match.group(1).split(',')]\n",
        "            group_by_cols_resolved = [self._resolve_aliases_in_expression(col, parsed.table_aliases) for col in group_by_cols]\n",
        "            group_by_part = f\".groupBy({', '.join(group_by_cols_resolved)})\"\n",
        "            chain_parts.append(group_by_part)\n",
        "\n",
        "            # Para GROUP BY, usar .agg() apenas com fun√ß√µes agregadas\n",
        "            # Extrair apenas fun√ß√µes agregadas do SELECT\n",
        "            columns = self._split_select_columns(parsed.select_clause)\n",
        "            agg_parts = []\n",
        "            for column in columns:\n",
        "                column_strip = column.strip()\n",
        "                if re.match(r'(?i)(AVG|SUM|COUNT|MIN|MAX)\\s*\\(', column_strip):\n",
        "                    # Fun√ß√£o agregada\n",
        "                    func_match = re.match(r'(?i)(AVG|SUM|COUNT|MIN|MAX)\\s*\\(([^)]+)\\)', column_strip)\n",
        "                    if func_match:\n",
        "                        func = func_match.group(1).lower()\n",
        "                        arg = func_match.group(2).strip()\n",
        "                        alias_name = None\n",
        "                        if ' as ' in column_strip.lower():\n",
        "                            parts = re.split(r'\\s+as\\s+', column_strip, flags=re.IGNORECASE)\n",
        "                            if len(parts) == 2:\n",
        "                                alias_name = parts[1].strip()\n",
        "                        arg_resolved = self._resolve_aliases_in_expression(arg, parsed.table_aliases)\n",
        "                        code = f\"F.{func}({arg_resolved})\"\n",
        "                        if alias_name:\n",
        "                            code += f\".alias('{alias_name}')\"\n",
        "                        agg_parts.append(code)\n",
        "                # N√£o adicionar colunas de group by no agg\n",
        "            agg_part = f\".agg({', '.join(agg_parts)})\"\n",
        "            chain_parts.append(agg_part)\n",
        "        else:\n",
        "            # Adicionar SELECT normalmente\n",
        "            select_columns = self._translate_select(parsed.select_clause, parsed.table_aliases)\n",
        "            select_part = f\".select({select_columns})\"\n",
        "            chain_parts.append(select_part)\n",
        "\n",
        "        # Adicionar ORDER BY\n",
        "        if parsed.order_by_clause:\n",
        "            order_columns = self._translate_order_by(parsed.order_by_clause, parsed.table_aliases)\n",
        "            order_part = f\".orderBy({order_columns})\"\n",
        "            chain_parts.append(order_part)\n",
        "\n",
        "        # Adicionar LIMIT se existir na query\n",
        "        limit_match = re.search(r'LIMIT\\s+(\\d+)', parsed.original_query, re.IGNORECASE)\n",
        "        if limit_match:\n",
        "            limit_n = int(limit_match.group(1))\n",
        "            chain_parts.append(f\".limit({limit_n})\")\n",
        "\n",
        "        # Juntar tudo em uma √∫nica string\n",
        "        return \"df = \" + \"\".join(chain_parts)\n",
        "\n",
        "    def _translate_join_for_chain(self, join_clause: str, aliases: Dict[str, str]) -> str:\n",
        "        \"\"\"Traduzir cl√°usula JOIN para method chaining.\"\"\"\n",
        "        # Extrair informa√ß√µes do JOIN\n",
        "        match = re.match(r'(\\w+)\\s+JOIN\\s+([\\w\\.]+)\\s+ON\\s+(.*)', join_clause, re.IGNORECASE)\n",
        "        if not match:\n",
        "            return f\".join(spark.table('ERROR'), F.lit(True), 'inner')\"\n",
        "\n",
        "        join_type = match.group(1).lower()\n",
        "        table_name = match.group(2)\n",
        "        condition = match.group(3)\n",
        "\n",
        "        # Resolver aliases na condi√ß√£o\n",
        "        condition_resolved = self._resolve_aliases_in_expression(condition, aliases)\n",
        "\n",
        "        # Mapear tipo de JOIN\n",
        "        join_map = {\n",
        "            'inner': 'inner',\n",
        "            'left': 'left',\n",
        "            'right': 'right',\n",
        "            'full': 'full'\n",
        "        }\n",
        "\n",
        "        pyspark_join_type = join_map.get(join_type, 'inner')\n",
        "\n",
        "        return f\".join(spark.table('{table_name}'), {condition_resolved}, '{pyspark_join_type}')\"\n",
        "\n",
        "    def _translate_join(self, join_clause: str, aliases: Dict[str, str]) -> str:\n",
        "        \"\"\"Traduzir cl√°usula JOIN.\"\"\"\n",
        "        # Extrair informa√ß√µes do JOIN\n",
        "        match = re.match(r'(\\w+)\\s+JOIN\\s+([\\w\\.]+)\\s+ON\\s+(.*)', join_clause, re.IGNORECASE)\n",
        "        if not match:\n",
        "            return f\"# ERRO: JOIN n√£o reconhecido: {join_clause}\"\n",
        "\n",
        "        join_type = match.group(1).lower()\n",
        "        table_name = match.group(2)\n",
        "        condition = match.group(3)\n",
        "\n",
        "        # Resolver aliases na condi√ß√£o\n",
        "        condition_resolved = self._resolve_aliases_in_expression(condition, aliases)\n",
        "\n",
        "        # Mapear tipo de JOIN\n",
        "        join_map = {\n",
        "            'inner': 'inner',\n",
        "            'left': 'left',\n",
        "            'right': 'right',\n",
        "            'full': 'full'\n",
        "        }\n",
        "\n",
        "        pyspark_join_type = join_map.get(join_type, 'inner')\n",
        "\n",
        "        return f\"df = df.join(spark.table('{table_name}'), {condition_resolved}, '{pyspark_join_type}')\"\n",
        "\n",
        "    def _translate_where(self, where_clause: str, aliases: Dict[str, str]) -> str:\n",
        "        \"\"\"Traduzir cl√°usula WHERE.\"\"\"\n",
        "        return self._resolve_aliases_in_expression(where_clause, aliases)\n",
        "\n",
        "    def _translate_select(self, select_clause: str, aliases: Dict[str, str]) -> str:\n",
        "        \"\"\"Traduzir cl√°usula SELECT.\"\"\"\n",
        "        if select_clause.strip() == '*':\n",
        "            return \"'*'\"\n",
        "\n",
        "        # Dividir colunas\n",
        "        columns = self._split_select_columns(select_clause)\n",
        "        select_parts = []\n",
        "\n",
        "        for column in columns:\n",
        "            column = column.strip()\n",
        "\n",
        "            # Verificar COALESCE\n",
        "            if 'COALESCE' in column.upper():\n",
        "                coalesce_code = self._translate_coalesce(column, aliases)\n",
        "                select_parts.append(coalesce_code)\n",
        "            # Verificar CASE WHEN\n",
        "            elif 'CASE' in column.upper():\n",
        "                case_code = self._translate_case_when(column, aliases)\n",
        "                select_parts.append(case_code)\n",
        "            # Verificar fun√ß√µes agregadas\n",
        "            elif re.match(r'(?i)(AVG|SUM|COUNT|MIN|MAX)\\s*\\(', column):\n",
        "                # Extrair fun√ß√£o e argumento\n",
        "                func_match = re.match(r'(?i)(AVG|SUM|COUNT|MIN|MAX)\\s*\\(([^)]+)\\)', column)\n",
        "                if func_match:\n",
        "                    func = func_match.group(1).lower()\n",
        "                    arg = func_match.group(2).strip()\n",
        "                    # Verificar alias\n",
        "                    alias_name = None\n",
        "                    if ' as ' in column.lower():\n",
        "                        parts = re.split(r'\\s+as\\s+', column, flags=re.IGNORECASE)\n",
        "                        if len(parts) == 2:\n",
        "                            alias_name = parts[1].strip()\n",
        "                    # Traduzir argumento\n",
        "                    arg_resolved = self._resolve_aliases_in_expression(arg, aliases)\n",
        "                    code = f\"F.{func}({arg_resolved})\"\n",
        "                    if alias_name:\n",
        "                        code += f\".alias('{alias_name}')\"\n",
        "                    select_parts.append(code)\n",
        "                else:\n",
        "                    select_parts.append(f\"F.lit('{column}')\")\n",
        "            # Coluna regular\n",
        "            else:\n",
        "                if ' as ' in column.lower():\n",
        "                    parts = re.split(r'\\s+as\\s+', column, flags=re.IGNORECASE)\n",
        "                    if len(parts) == 2:\n",
        "                        col_expr = self._resolve_aliases_in_expression(parts[0].strip(), aliases)\n",
        "                        alias_name = parts[1].strip()\n",
        "                        select_parts.append(f\"{col_expr}.alias('{alias_name}')\")\n",
        "                    else:\n",
        "                        resolved = self._resolve_aliases_in_expression(column, aliases)\n",
        "                        select_parts.append(resolved)\n",
        "                else:\n",
        "                    resolved = self._resolve_aliases_in_expression(column, aliases)\n",
        "                    select_parts.append(resolved)\n",
        "\n",
        "        return \", \".join(select_parts)\n",
        "\n",
        "    def _translate_order_by(self, order_clause: str, aliases: Dict[str, str]) -> str:\n",
        "        \"\"\"Traduzir cl√°usula ORDER BY.\"\"\"\n",
        "        columns = [col.strip() for col in order_clause.split(',')]\n",
        "        order_parts = []\n",
        "\n",
        "        for column in columns:\n",
        "            if column.upper().endswith(' DESC'):\n",
        "                col_name = column[:-5].strip()\n",
        "                resolved = self._resolve_aliases_in_expression(col_name, aliases)\n",
        "                order_parts.append(f\"{resolved}.desc()\")\n",
        "            elif column.upper().endswith(' ASC'):\n",
        "                col_name = column[:-4].strip()\n",
        "                resolved = self._resolve_aliases_in_expression(col_name, aliases)\n",
        "                order_parts.append(f\"{resolved}.asc()\")\n",
        "            else:\n",
        "                resolved = self._resolve_aliases_in_expression(column, aliases)\n",
        "                order_parts.append(f\"{resolved}.asc()\")\n",
        "\n",
        "        return \", \".join(order_parts)\n",
        "\n",
        "    def _translate_coalesce(self, coalesce_expr: str, aliases: Dict[str, str]) -> str:\n",
        "        \"\"\"Traduzir express√£o COALESCE.\"\"\"\n",
        "        # Extrair argumentos do COALESCE\n",
        "        match = re.search(r'COALESCE\\s*\\(([^)]+)\\)', coalesce_expr, re.IGNORECASE)\n",
        "        if not match:\n",
        "            return f\"F.lit('{coalesce_expr}')\"\n",
        "\n",
        "        args_str = match.group(1)\n",
        "\n",
        "        # Dividir argumentos respeitando aspas\n",
        "        args = self._split_function_args(args_str)\n",
        "        resolved_args = []\n",
        "\n",
        "        for arg in args:\n",
        "            arg = arg.strip()\n",
        "            if arg.startswith(\"'\") and arg.endswith(\"'\"):\n",
        "                # String literal\n",
        "                resolved_args.append(f\"F.lit({arg})\")\n",
        "            else:\n",
        "                # Coluna\n",
        "                resolved = self._resolve_aliases_in_expression(arg, aliases)\n",
        "                resolved_args.append(resolved)\n",
        "\n",
        "        result = f\"F.coalesce({', '.join(resolved_args)})\"\n",
        "\n",
        "        # Verificar alias\n",
        "        if ' as ' in coalesce_expr.lower():\n",
        "            parts = re.split(r'\\s+as\\s+', coalesce_expr, flags=re.IGNORECASE)\n",
        "            if len(parts) == 2:\n",
        "                alias_name = parts[1].strip()\n",
        "                result += f\".alias('{alias_name}')\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _translate_case_when(self, case_expr: str, aliases: Dict[str, str]) -> str:\n",
        "        \"\"\"Traduzir express√£o CASE WHEN.\"\"\"\n",
        "        # Simplificado: retornar como literal por ora\n",
        "        # Em implementa√ß√£o completa, seria necess√°rio parser mais sofisticado\n",
        "        resolved = self._resolve_aliases_in_expression(case_expr, aliases)\n",
        "        return f\"F.lit('{resolved}')\"\n",
        "\n",
        "    def _resolve_aliases_in_expression(self, expression: str, aliases: Dict[str, str]) -> str:\n",
        "        \"\"\"Resolver aliases em uma express√£o, substituindo por nomes reais de tabelas.\"\"\"\n",
        "        if not aliases:\n",
        "            # Se n√£o h√° aliases, assumir que √© uma coluna simples\n",
        "            if '.' not in expression:\n",
        "                return f\"F.col('{expression.strip().rstrip(';')}')\"\n",
        "            else:\n",
        "                return f\"F.col('{expression.strip().rstrip(';')}')\"\n",
        "\n",
        "        result = expression\n",
        "\n",
        "        # Substituir todos os aliases por nomes reais de tabela\n",
        "        # Ordenar por tamanho do alias para evitar conflitos de prefixo\n",
        "        for alias in sorted(aliases, key=len, reverse=True):\n",
        "            real_name = aliases[alias]\n",
        "            # Substituir alias.coluna por real_name.coluna\n",
        "            pattern = rf'\\b{re.escape(alias)}\\.(\\w+)'\n",
        "            result = re.sub(pattern, rf'{real_name}.\\1', result)\n",
        "\n",
        "        # Remover ponto e v√≠rgula ao final de nomes de colunas\n",
        "        result = result.strip().rstrip(';')\n",
        "\n",
        "        # Converter para F.col() se necess√°rio\n",
        "        if not result.startswith('F.') and not result.startswith('('):\n",
        "            # Se parece com uma refer√™ncia de coluna simples\n",
        "            if '=' in result:\n",
        "                # √â uma condi√ß√£o de igualdade\n",
        "                parts = result.split('=')\n",
        "                if len(parts) == 2:\n",
        "                    left = parts[0].strip().rstrip(';')\n",
        "                    right = parts[1].strip().rstrip(';')\n",
        "\n",
        "                    # Converter lado esquerdo\n",
        "                    if '.' in left:\n",
        "                        left_col = f\"F.col('{left}')\"\n",
        "                    else:\n",
        "                        left_col = f\"F.col('{left}')\"\n",
        "\n",
        "                    # Converter lado direito - detectar valores literais\n",
        "                    if right.isdigit() or (right.replace('.', '').isdigit() and right.count('.') <= 1):\n",
        "                        # √â um n√∫mero\n",
        "                        right_col = f\"F.lit({right})\"\n",
        "                    elif right.startswith(\"'\") and right.endswith(\"'\"):\n",
        "                        # √â uma string\n",
        "                        right_col = f\"F.lit({right})\"\n",
        "                    elif '.' in right:\n",
        "                        # √â uma coluna com qualificador de tabela\n",
        "                        right_col = f\"F.col('{right}')\"\n",
        "                    else:\n",
        "                        # √â uma coluna simples\n",
        "                        right_col = f\"F.col('{right}')\"\n",
        "\n",
        "                    result = f\"{left_col} == {right_col}\"\n",
        "                else:\n",
        "                    result = f\"F.col('{result}')\"\n",
        "            elif '>' in result or '<' in result:\n",
        "                # √â uma condi√ß√£o de compara√ß√£o\n",
        "                for op in ['>=', '<=', '>', '<', '!=']:\n",
        "                    if op in result:\n",
        "                        parts = result.split(op)\n",
        "                        if len(parts) == 2:\n",
        "                            left = parts[0].strip().rstrip(';')\n",
        "                            right = parts[1].strip().rstrip(';')\n",
        "\n",
        "                            # Converter lado esquerdo\n",
        "                            if '.' in left:\n",
        "                                left_col = f\"F.col('{left}')\"\n",
        "                            else:\n",
        "                                left_col = f\"F.col('{left}')\"\n",
        "\n",
        "                            # Converter lado direito - detectar valores literais\n",
        "                            if right.isdigit() or (right.replace('.', '').isdigit() and right.count('.') <= 1):\n",
        "                                # √â um n√∫mero\n",
        "                                right_col = f\"F.lit({right})\"\n",
        "                            elif right.startswith(\"'\") and right.endswith(\"'\"):\n",
        "                                # √â uma string\n",
        "                                right_col = f\"F.lit({right})\"\n",
        "                            elif '.' in right:\n",
        "                                # √â uma coluna com qualificador de tabela\n",
        "                                right_col = f\"F.col('{right}')\"\n",
        "                            else:\n",
        "                                # √â uma coluna simples\n",
        "                                right_col = f\"F.col('{right}')\"\n",
        "\n",
        "                            py_op = '==' if op == '=' else op\n",
        "                            result = f\"{left_col} {py_op} {right_col}\"\n",
        "                            break\n",
        "                else:\n",
        "                    result = f\"F.col('{result}')\"\n",
        "            else:\n",
        "                # √â uma refer√™ncia de coluna simples\n",
        "                result = f\"F.col('{result}')\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _split_select_columns(self, select_clause: str) -> List[str]:\n",
        "        \"\"\"Dividir colunas SELECT respeitando par√™nteses e aspas.\"\"\"\n",
        "        columns = []\n",
        "        current_column = \"\"\n",
        "        paren_count = 0\n",
        "        in_quotes = False\n",
        "        quote_char = None\n",
        "\n",
        "        for char in select_clause:\n",
        "            if char in [\"'\", '\"'] and not in_quotes:\n",
        "                in_quotes = True\n",
        "                quote_char = char\n",
        "                current_column += char\n",
        "            elif char == quote_char and in_quotes:\n",
        "                in_quotes = False\n",
        "                quote_char = None\n",
        "                current_column += char\n",
        "            elif char == '(' and not in_quotes:\n",
        "                paren_count += 1\n",
        "                current_column += char\n",
        "            elif char == ')' and not in_quotes:\n",
        "                paren_count -= 1\n",
        "                current_column += char\n",
        "            elif char == ',' and paren_count == 0 and not in_quotes:\n",
        "                if current_column.strip():\n",
        "                    columns.append(current_column.strip())\n",
        "                current_column = \"\"\n",
        "            else:\n",
        "                current_column += char\n",
        "\n",
        "        if current_column.strip():\n",
        "            columns.append(current_column.strip())\n",
        "\n",
        "        return columns\n",
        "\n",
        "    def _split_function_args(self, args_str: str) -> List[str]:\n",
        "        \"\"\"Dividir argumentos de fun√ß√£o respeitando aspas.\"\"\"\n",
        "        args = []\n",
        "        current_arg = \"\"\n",
        "        in_quotes = False\n",
        "        quote_char = None\n",
        "\n",
        "        for char in args_str:\n",
        "            if char in [\"'\", '\"'] and not in_quotes:\n",
        "                in_quotes = True\n",
        "                quote_char = char\n",
        "                current_arg += char\n",
        "            elif char == quote_char and in_quotes:\n",
        "                in_quotes = False\n",
        "                quote_char = None\n",
        "                current_arg += char\n",
        "            elif char == ',' and not in_quotes:\n",
        "                if current_arg.strip():\n",
        "                    args.append(current_arg.strip())\n",
        "                current_arg = \"\"\n",
        "            else:\n",
        "                current_arg += char\n",
        "\n",
        "        if current_arg.strip():\n",
        "            args.append(current_arg.strip())\n",
        "\n",
        "        return args\n",
        "\n",
        "    def _validate_no_aliases(self, code: str, aliases: Dict[str, str]) -> Dict:\n",
        "        \"\"\"Validar que o c√≥digo n√£o cont√©m aliases de tabelas.\"\"\"\n",
        "        found_aliases = []\n",
        "\n",
        "        for alias in aliases.keys():\n",
        "            # Procurar por padr√£o alias.coluna no c√≥digo\n",
        "            pattern = r'\\b' + re.escape(alias) + r'\\.\\w+'\n",
        "            matches = re.findall(pattern, code)\n",
        "            if matches:\n",
        "                found_aliases.extend(matches)\n",
        "\n",
        "        return {\n",
        "            'passed': len(found_aliases) == 0,\n",
        "            'found_aliases': found_aliases,\n",
        "            'message': 'Valida√ß√£o passou: nenhum alias encontrado' if len(found_aliases) == 0 else f'Aliases encontrados: {found_aliases}'\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Tradutor PySpark criado com sucesso!\")\n",
        "print(\"   ‚ö° Tradu√ß√£o SQL ‚Üí PySpark\")\n",
        "print(\"   üéØ Resolu√ß√£o autom√°tica de aliases\")\n",
        "print(\"   üìä Suporte: JOIN, WHERE, ORDER BY, COALESCE\")\n",
        "print(\"   ‚úÖ Valida√ß√£o autom√°tica\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec09bfb6"
      },
      "source": [
        "## üß™ 4. Testes e Valida√ß√£o\n",
        "\n",
        "Testes que demonstram o funcionamento correto do tradutor:\n",
        "- Tradu√ß√£o de consultas com JOINs\n",
        "- Resolu√ß√£o correta de aliases\n",
        "- Suporte a COALESCE\n",
        "- Valida√ß√£o autom√°tica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fa985e29",
        "outputId": "9bb20eea-fba2-4563-da24-e085b32cb230",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ EXECUTANDO TESTES DO TRADUTOR\n",
            "============================================================\n",
            "\n",
            "üß™ Teste 1: JOIN com aliases\n",
            "----------------------------------------\n",
            "‚úÖ Status: SUCESSO\n",
            "\n",
            "üìù SQL Original:\n",
            "   SELECT f.nome, d.nome as departamento, f.salario\n",
            "               FROM funcionarios f\n",
            "               INNER JOIN departamentos d ON f.departamento_id = d.id\n",
            "               WHERE f.ativo = 1\n",
            "               ORDER BY f.salario DESC\n",
            "\n",
            "üîß C√≥digo PySpark Gerado:\n",
            "   df = spark.table('funcionarios').join(spark.table('departamentos'), F.col('funcionarios.departamento_id') == F.col('departamentos.id'), 'inner').filter(F.col('funcionarios.ativo') == F.lit(1)).select(F.col('funcionarios.nome'), F.col('departamentos.nome').alias('departamento'), F.col('funcionarios.salario')).orderBy(F.col('funcionarios.salario').desc())\n",
            "\n",
            "üéØ Aliases Detectados:\n",
            "   f ‚Üí funcionarios\n",
            "   d ‚Üí departamentos\n",
            "\n",
            "‚úÖ Valida√ß√£o de Aliases:\n",
            "   ‚úÖ PASSOU: Nenhum alias encontrado no c√≥digo PySpark\n",
            "\n",
            "============================================================\n",
            "\n",
            "üß™ Teste 2: COALESCE com aliases\n",
            "----------------------------------------\n",
            "‚úÖ Status: SUCESSO\n",
            "\n",
            "üìù SQL Original:\n",
            "   SELECT f.nome,\n",
            "                      COALESCE(f.email, f.telefone, 'Sem contato') as contato\n",
            "               FROM funcionarios f\n",
            "               WHERE f.ativo = 1\n",
            "\n",
            "üîß C√≥digo PySpark Gerado:\n",
            "   df = spark.table('funcionarios').filter(F.col('funcionarios.ativo') == F.lit(1)).select(F.col('funcionarios.nome'), F.coalesce(F.col('funcionarios.email'), F.col('funcionarios.telefone'), F.lit('Sem contato')).alias('contato'))\n",
            "\n",
            "üéØ Aliases Detectados:\n",
            "   f ‚Üí funcionarios\n",
            "\n",
            "‚úÖ Valida√ß√£o de Aliases:\n",
            "   ‚úÖ PASSOU: Nenhum alias encontrado no c√≥digo PySpark\n",
            "\n",
            "============================================================\n",
            "\n",
            "üß™ Teste 3: Consulta simples sem aliases\n",
            "----------------------------------------\n",
            "‚úÖ Status: SUCESSO\n",
            "\n",
            "üìù SQL Original:\n",
            "   SELECT nome, salario\n",
            "               FROM funcionarios\n",
            "               WHERE ativo = 1\n",
            "               ORDER BY nome\n",
            "\n",
            "üîß C√≥digo PySpark Gerado:\n",
            "   df = spark.table('funcionarios').filter(F.col('ativo') == F.lit(1)).select(F.col('nome'), F.col('salario')).orderBy(F.col('nome').asc())\n",
            "\n",
            "üéØ Aliases Detectados:\n",
            "   WHERE ‚Üí funcionarios\n",
            "\n",
            "‚úÖ Valida√ß√£o de Aliases:\n",
            "   ‚úÖ PASSOU: Nenhum alias encontrado no c√≥digo PySpark\n",
            "\n",
            "============================================================\n",
            "\n",
            "üéâ TESTES CONCLU√çDOS!\n"
          ]
        }
      ],
      "source": [
        "# üß™ TESTES E VALIDA√á√ÉO\n",
        "\n",
        "def test_translator():\n",
        "    \"\"\"Executar testes do tradutor.\"\"\"\n",
        "    translator = SimpleSQLTranslator()\n",
        "\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"name\": \"JOIN com aliases\",\n",
        "            \"sql\": \"\"\"\n",
        "            SELECT f.nome, d.nome as departamento, f.salario\n",
        "            FROM funcionarios f\n",
        "            INNER JOIN departamentos d ON f.departamento_id = d.id\n",
        "            WHERE f.ativo = 1\n",
        "            ORDER BY f.salario DESC\n",
        "            \"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"COALESCE com aliases\",\n",
        "            \"sql\": \"\"\"\n",
        "            SELECT f.nome,\n",
        "                   COALESCE(f.email, f.telefone, 'Sem contato') as contato\n",
        "            FROM funcionarios f\n",
        "            WHERE f.ativo = 1\n",
        "            \"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Consulta simples sem aliases\",\n",
        "            \"sql\": \"\"\"\n",
        "            SELECT nome, salario\n",
        "            FROM funcionarios\n",
        "            WHERE ativo = 1\n",
        "            ORDER BY nome\n",
        "            \"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    print(\"üöÄ EXECUTANDO TESTES DO TRADUTOR\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i, test_case in enumerate(test_cases, 1):\n",
        "        print(f\"\\nüß™ Teste {i}: {test_case['name']}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Traduzir\n",
        "        result = translator.translate(test_case['sql'])\n",
        "\n",
        "        if result['success']:\n",
        "            print(\"‚úÖ Status: SUCESSO\")\n",
        "\n",
        "            print(\"\\nüìù SQL Original:\")\n",
        "            print(textwrap.indent(test_case['sql'].strip(), \"   \"))\n",
        "\n",
        "            print(\"\\nüîß C√≥digo PySpark Gerado:\")\n",
        "            print(textwrap.indent(result['pyspark_code'], \"   \"))\n",
        "\n",
        "            print(\"\\nüéØ Aliases Detectados:\")\n",
        "            if result['table_aliases']:\n",
        "                for alias, real_name in result['table_aliases'].items():\n",
        "                    print(f\"   {alias} ‚Üí {real_name}\")\n",
        "            else:\n",
        "                print(\"   Nenhum alias detectado\")\n",
        "\n",
        "            print(\"\\n‚úÖ Valida√ß√£o de Aliases:\")\n",
        "            validation = result['validation']\n",
        "            if validation['passed']:\n",
        "                print(\"   ‚úÖ PASSOU: Nenhum alias encontrado no c√≥digo PySpark\")\n",
        "            else:\n",
        "                print(f\"   ‚ùå FALHOU: {validation['message']}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"‚ùå Status: ERRO - {result['error']}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    print(\"\\nüéâ TESTES CONCLU√çDOS!\")\n",
        "\n",
        "# Executar testes\n",
        "test_translator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f52431ae"
      },
      "source": [
        "## üí° 5. Uso Pr√°tico\n",
        "\n",
        "Fun√ß√£o para traduzir suas pr√≥prias consultas SQL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "05aa9c37",
        "outputId": "923b026f-fa98-4cce-9b6e-861a0014ca97",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã Exemplo de tradu√ß√£o:\n",
            "üîÑ TRADUZINDO CONSULTA SQL\n",
            "==================================================\n",
            "‚úÖ Tradu√ß√£o realizada com sucesso!\n",
            "\n",
            "üìù SQL Original:\n",
            "   SELECT e.nome, d.nome as departamento, e.salario,\n",
            "          COALESCE(e.email, 'sem-email@empresa.com') as email_contato\n",
            "   FROM empregados e\n",
            "   LEFT JOIN departamentos d ON e.dept_id = d.id\n",
            "   WHERE e.ativo = 1 AND e.salario > 3000\n",
            "   ORDER BY e.salario DESC\n",
            "\n",
            "üîß C√≥digo PySpark:\n",
            "   df = spark.table('empregados').join(spark.table('departamentos'), F.col('empregados.dept_id') == F.col('departamentos.id'), 'left').filter(F.col('empregados.ativo') == F.col('1 AND empregados.salario > 3000')).select(F.col('empregados.nome'), F.col('departamentos.nome').alias('departamento'), F.col('empregados.salario'), F.coalesce(F.col('empregados.email'), F.lit('sem-email@empresa.com')).alias('email_contato')).orderBy(F.col('empregados.salario').desc())\n",
            "\n",
            "üéØ Informa√ß√µes:\n",
            "   Aliases: 2 detectados\n",
            "   Valida√ß√£o: ‚úÖ PASSOU\n",
            "\n",
            "\n",
            "üí° Para traduzir suas pr√≥prias consultas, use:\n",
            "   traduzir_sql('SEU_SQL_AQUI')\n"
          ]
        }
      ],
      "source": [
        "# üí° USO PR√ÅTICO\n",
        "\n",
        "def traduzir_sql(sql_query: str):\n",
        "    \"\"\"Fun√ß√£o para traduzir consultas SQL personalizadas.\"\"\"\n",
        "    translator = SimpleSQLTranslator()\n",
        "\n",
        "    print(\"üîÑ TRADUZINDO CONSULTA SQL\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    result = translator.translate(sql_query)\n",
        "\n",
        "    if result['success']:\n",
        "        print(\"‚úÖ Tradu√ß√£o realizada com sucesso!\")\n",
        "        print()\n",
        "\n",
        "        print(\"üìù SQL Original:\")\n",
        "        print(textwrap.indent(sql_query.strip(), \"   \"))\n",
        "        print()\n",
        "\n",
        "        print(\"üîß C√≥digo PySpark:\")\n",
        "        print(textwrap.indent(result['pyspark_code'], \"   \"))\n",
        "        print()\n",
        "\n",
        "        print(\"üéØ Informa√ß√µes:\")\n",
        "        print(f\"   Aliases: {len(result['table_aliases'])} detectados\")\n",
        "        validation = result['validation']\n",
        "        print(f\"   Valida√ß√£o: {'‚úÖ PASSOU' if validation['passed'] else '‚ùå FALHOU'}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"‚ùå Erro na tradu√ß√£o: {result['error']}\")\n",
        "\n",
        "    print()\n",
        "    return result\n",
        "\n",
        "# Exemplo de uso\n",
        "sql_exemplo = \"\"\"\n",
        "SELECT e.nome, d.nome as departamento, e.salario,\n",
        "       COALESCE(e.email, 'sem-email@empresa.com') as email_contato\n",
        "FROM empregados e\n",
        "LEFT JOIN departamentos d ON e.dept_id = d.id\n",
        "WHERE e.ativo = 1 AND e.salario > 3000\n",
        "ORDER BY e.salario DESC\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìã Exemplo de tradu√ß√£o:\")\n",
        "resultado = traduzir_sql(sql_exemplo)\n",
        "\n",
        "print(\"\\nüí° Para traduzir suas pr√≥prias consultas, use:\")\n",
        "print(\"   traduzir_sql('SEU_SQL_AQUI')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "v0JmL30hT452",
        "outputId": "aaa779ee-1661-476f-b8d8-b07fb8f2a3b5",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DIVERSOS TESTES PR√ÅTICOS\n",
        "\n",
        "* Para verificar a utiliza√ß√£o do c√≥digo em Pyspark em tabelas reais."
      ],
      "metadata": {
        "id": "pqUKNNNQgoJu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f74f6db2",
        "outputId": "ce48507a-4b79-4637-f254-a906ea7ccc34",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabelas de exemplo criadas!\n",
            "funcionarios:\n",
            "+---+------+-------------+-------+-----+---------------+\n",
            "| id|  nome|        cargo|salario|ativo|departamento_id|\n",
            "+---+------+-------------+-------+-----+---------------+\n",
            "|  1| Alice|     Analista|5000.00| TRUE|              1|\n",
            "|  2|   Bob|Desenvolvedor|7000.00| TRUE|              2|\n",
            "|  3|Carlos|      Gerente|9000.00| TRUE|              1|\n",
            "|  4| Diana|Desenvolvedor|6500.00| TRUE|              2|\n",
            "|  5|   Eva|     Analista|4800.00|FALSE|              3|\n",
            "+---+------+-------------+-------+-----+---------------+\n",
            "\n",
            "departamento:\n",
            "+---+----------------+--------------+\n",
            "| id|            nome|   localizacao|\n",
            "+---+----------------+--------------+\n",
            "|  1|Recursos Humanos|     S√£o Paulo|\n",
            "|  2| Desenvolvimento|Rio de Janeiro|\n",
            "|  3|       Marketing|      Curitiba|\n",
            "+---+----------------+--------------+\n",
            "\n",
            "avaliacoes:\n",
            "+---+--------------+----------+--------------+\n",
            "| id|funcionario_id|projeto_id|horas_semanais|\n",
            "+---+--------------+----------+--------------+\n",
            "|  1|             1|         3|            20|\n",
            "|  2|             2|         1|            30|\n",
            "|  3|             3|         3|            10|\n",
            "|  4|             4|         1|            25|\n",
            "|  5|             5|         2|            15|\n",
            "+---+--------------+----------+--------------+\n",
            "\n",
            "projetos:\n",
            "+---+-------------+--------------------+---------------+\n",
            "| id|         nome|           descricao|departamento_id|\n",
            "+---+-------------+--------------------+---------------+\n",
            "|  1|Projeto Alpha|Desenvolvimento d...|              2|\n",
            "|  2| Projeto Beta|Campanha de marke...|              3|\n",
            "|  3|Projeto Gamma|Reestrutura√ß√£o de RH|              1|\n",
            "+---+-------------+--------------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Criar DataFrames de exemplo\n",
        "import pandas as pd\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Dados de exemplos\n",
        "df_funcionarios = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/Spark/funcionarios.csv', header=True)\n",
        "df_departamento = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/Spark/departamento.csv', header=True)\n",
        "df_avaliacoes = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/Spark/avaliacoes.csv', header=True)\n",
        "df_projetos = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/Spark/projetos.csv', header=True)\n",
        "\n",
        "# Dados de exemplo para tabelas importadas\n",
        "df_funcionarios.createOrReplaceTempView(\"funcionarios\")\n",
        "df_departamento.createOrReplaceTempView(\"departamento\")\n",
        "df_avaliacoes.createOrReplaceTempView(\"avaliacoes\")\n",
        "df_projetos.createOrReplaceTempView(\"projetos\")\n",
        "\n",
        "print(\"Tabelas de exemplo criadas!\")\n",
        "print('funcionarios:')\n",
        "df_funcionarios.show()\n",
        "print('departamento:')\n",
        "df_departamento.show()\n",
        "print('avaliacoes:')\n",
        "df_avaliacoes.show()\n",
        "print('projetos:')\n",
        "df_projetos.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f3491798",
        "outputId": "9b5eeb8a-cdbb-4758-87c3-14d44581f5a5",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç --- Consulta SQL Original ---\n",
            "\n",
            "SELECT nome, salario\n",
            "FROM funcionarios\n",
            "WHERE ativo = 'TRUE'\n",
            "ORDER BY salario DESC\n",
            "\n",
            "\n",
            "‚ö° --- Resultado SparkSQL ---\n",
            "+------+-------+\n",
            "|  nome|salario|\n",
            "+------+-------+\n",
            "|Carlos|9000.00|\n",
            "|   Bob|7000.00|\n",
            "| Diana|6500.00|\n",
            "| Alice|5000.00|\n",
            "+------+-------+\n",
            "\n",
            "üîÑ --- Traduzindo para PySpark ---\n",
            "‚úÖ Tradu√ß√£o bem-sucedida!\n",
            "\n",
            "üîß --- C√≥digo PySpark Gerado ---\n",
            "df = spark.table('funcionarios').filter(F.col('ativo') == F.lit('TRUE')).select(F.col('nome'), F.col('salario')).orderBy(F.col('salario').desc())\n",
            "\n",
            "üìä --- Resultado PySpark ---\n",
            "+------+-------+\n",
            "|  nome|salario|\n",
            "+------+-------+\n",
            "|Carlos|9000.00|\n",
            "|   Bob|7000.00|\n",
            "| Diana|6500.00|\n",
            "| Alice|5000.00|\n",
            "+------+-------+\n",
            "\n",
            "üéØ --- Valida√ß√£o ---\n",
            "‚úÖ Valida√ß√£o passou: Nenhum alias encontrado no c√≥digo PySpark\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# üß™ EXEMPLO PR√ÅTICO: Consulta Simples com Compara√ß√£o SparkSQL vs PySpark\n",
        "sql_exemplo = '''\n",
        "SELECT nome, salario\n",
        "FROM funcionarios\n",
        "WHERE ativo = 'TRUE'\n",
        "ORDER BY salario DESC\n",
        "'''\n",
        "\n",
        "print('üîç --- Consulta SQL Original ---')\n",
        "print(sql_exemplo)\n",
        "print()\n",
        "\n",
        "# Executar com SparkSQL\n",
        "print('‚ö° --- Resultado SparkSQL ---')\n",
        "df_sql = spark.sql(sql_exemplo)\n",
        "df_sql.show()\n",
        "\n",
        "# Traduzir para PySpark\n",
        "print('üîÑ --- Traduzindo para PySpark ---')\n",
        "translator = SimpleSQLTranslator()\n",
        "resultado = translator.translate(sql_exemplo)\n",
        "\n",
        "if resultado['success']:\n",
        "    print('‚úÖ Tradu√ß√£o bem-sucedida!')\n",
        "    print()\n",
        "    print('üîß --- C√≥digo PySpark Gerado ---')\n",
        "    print(resultado['pyspark_code'])\n",
        "    print()\n",
        "\n",
        "    # Executar c√≥digo PySpark gerado\n",
        "    print('üìä --- Resultado PySpark ---')\n",
        "    try:\n",
        "        # Criar um ambiente local para execu√ß√£o\n",
        "        local_vars = {'spark': spark, 'F': F}\n",
        "        exec(resultado['pyspark_code'], globals(), local_vars)\n",
        "        df_pyspark = local_vars['df']\n",
        "        df_pyspark.show()\n",
        "\n",
        "        print('üéØ --- Valida√ß√£o ---')\n",
        "        validation = resultado['validation']\n",
        "        if validation['passed']:\n",
        "            print('‚úÖ Valida√ß√£o passou: Nenhum alias encontrado no c√≥digo PySpark')\n",
        "        else:\n",
        "            print(f'‚ùå Valida√ß√£o falhou: {validation[\"message\"]}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Erro ao executar c√≥digo PySpark: {e}')\n",
        "\n",
        "else:\n",
        "    print(f'‚ùå Erro na tradu√ß√£o: {resultado[\"error\"]}')\n",
        "\n",
        "print('\\n' + '='*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "86c2b399",
        "outputId": "240f143c-7dcc-4b8a-96d3-210f5032cc22",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç --- Consulta SQL Complexa ---\n",
            "\n",
            "SELECT f.nome, f.cargo, d.nome AS departamento\n",
            "FROM funcionarios f\n",
            "JOIN departamento d ON f.departamento_id = d.id;\n",
            "\n",
            "\n",
            "‚ö° --- Resultado SparkSQL ---\n",
            "spark.slq(\"\"\"\n",
            "SELECT f.nome, f.cargo, d.nome AS departamento\n",
            "FROM funcionarios f\n",
            "JOIN departamento d ON f.departamento_id = d.id;\n",
            "\"\"\")\n",
            "+------+-------------+----------------+\n",
            "|  nome|        cargo|    departamento|\n",
            "+------+-------------+----------------+\n",
            "| Alice|     Analista|Recursos Humanos|\n",
            "|   Bob|Desenvolvedor| Desenvolvimento|\n",
            "|Carlos|      Gerente|Recursos Humanos|\n",
            "| Diana|Desenvolvedor| Desenvolvimento|\n",
            "|   Eva|     Analista|       Marketing|\n",
            "+------+-------------+----------------+\n",
            "\n",
            "üîÑ --- Traduzindo SQL Complexo para PySpark ---\n",
            "‚úÖ Tradu√ß√£o bem-sucedida!\n",
            "\n",
            "üîß --- C√≥digo PySpark Gerado ---\n",
            "df = spark.table('funcionarios').join(spark.table('departamento'), F.col('funcionarios.departamento_id') == F.col('departamento.id'), 'inner').select(F.col('funcionarios.nome'), F.col('funcionarios.cargo'), F.col('departamento.nome').alias('departamento'))\n",
            "\n",
            "üéØ --- Aliases Detectados ---\n",
            "   f ‚Üí funcionarios\n",
            "   d ‚Üí departamento\n",
            "\n",
            "üìä --- Resultado PySpark ---\n",
            "+------+-------------+----------------+\n",
            "|  nome|        cargo|    departamento|\n",
            "+------+-------------+----------------+\n",
            "| Alice|     Analista|Recursos Humanos|\n",
            "|   Bob|Desenvolvedor| Desenvolvimento|\n",
            "|Carlos|      Gerente|Recursos Humanos|\n",
            "| Diana|Desenvolvedor| Desenvolvimento|\n",
            "|   Eva|     Analista|       Marketing|\n",
            "+------+-------------+----------------+\n",
            "\n",
            "üéØ --- Valida√ß√£o Final ---\n",
            "‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!\n",
            "‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas\n",
            "\n",
            "============================================================\n",
            "üéâ DEMONSTRA√á√ÉO COMPLETA!\n",
            "üí° O tradutor funciona corretamente para consultas simples e complexas\n",
            "üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas\n",
            "‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados\n"
          ]
        }
      ],
      "source": [
        "# üß™ Listar o nome do funcion√°rio, seu cargo e o nome do departamento:\n",
        "sql_complexo = '''\n",
        "SELECT f.nome, f.cargo, d.nome AS departamento\n",
        "FROM funcionarios f\n",
        "JOIN departamento d ON f.departamento_id = d.id;\n",
        "'''\n",
        "\n",
        "print('üîç --- Consulta SQL Complexa ---')\n",
        "print(sql_complexo)\n",
        "print()\n",
        "\n",
        "# Executar com SparkSQL\n",
        "print('‚ö° --- Resultado SparkSQL ---')\n",
        "print(f'spark.slq(\"\"\"{sql_complexo}\"\"\")')\n",
        "df_sql_complexo = spark.sql(sql_complexo)\n",
        "df_sql_complexo.show()\n",
        "\n",
        "# Traduzir para PySpark\n",
        "print('üîÑ --- Traduzindo SQL Complexo para PySpark ---')\n",
        "translator = SimpleSQLTranslator()\n",
        "resultado_complexo = translator.translate(sql_complexo)\n",
        "\n",
        "if resultado_complexo['success']:\n",
        "    print('‚úÖ Tradu√ß√£o bem-sucedida!')\n",
        "    print()\n",
        "    print('üîß --- C√≥digo PySpark Gerado ---')\n",
        "    print(resultado_complexo['pyspark_code'])\n",
        "    print()\n",
        "\n",
        "    print('üéØ --- Aliases Detectados ---')\n",
        "    if resultado_complexo['table_aliases']:\n",
        "        for alias, real_name in resultado_complexo['table_aliases'].items():\n",
        "            print(f'   {alias} ‚Üí {real_name}')\n",
        "    else:\n",
        "        print('   Nenhum alias detectado')\n",
        "    print()\n",
        "\n",
        "    # Executar c√≥digo PySpark gerado\n",
        "    print('üìä --- Resultado PySpark ---')\n",
        "    try:\n",
        "        local_vars = {'spark': spark, 'F': F}\n",
        "        exec(resultado_complexo['pyspark_code'], globals(), local_vars)\n",
        "        df_pyspark_complexo = local_vars['df']\n",
        "        df_pyspark_complexo.show()\n",
        "\n",
        "        print('üéØ --- Valida√ß√£o Final ---')\n",
        "        validation = resultado_complexo['validation']\n",
        "        if validation['passed']:\n",
        "            print('‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!')\n",
        "            print('‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas')\n",
        "        else:\n",
        "            print(f'‚ùå Valida√ß√£o falhou: {validation[\"message\"]}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Erro ao executar c√≥digo PySpark: {e}')\n",
        "\n",
        "else:\n",
        "    print(f'‚ùå Erro na tradu√ß√£o: {resultado_complexo[\"error\"]}')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('üéâ DEMONSTRA√á√ÉO COMPLETA!')\n",
        "print('üí° O tradutor funciona corretamente para consultas simples e complexas')\n",
        "print('üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas')\n",
        "print('‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a0EU39CrX0oH",
        "outputId": "cb178235-1182-4e59-c8eb-503b91366a7d",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç --- Consulta SQL Complexa ---\n",
            "\n",
            "SELECT cargo, AVG(salario) AS salario_medio\n",
            "FROM funcionarios\n",
            "GROUP BY cargo;\n",
            "\n",
            "\n",
            "‚ö° --- Resultado SparkSQL ---\n",
            "spark.slq(\"\"\"\n",
            "SELECT cargo, AVG(salario) AS salario_medio\n",
            "FROM funcionarios\n",
            "GROUP BY cargo;\n",
            "\"\"\")\n",
            "+-------------+-------------+\n",
            "|        cargo|salario_medio|\n",
            "+-------------+-------------+\n",
            "|      Gerente|       9000.0|\n",
            "|Desenvolvedor|       6750.0|\n",
            "|     Analista|       4900.0|\n",
            "+-------------+-------------+\n",
            "\n",
            "üîÑ --- Traduzindo SQL Complexo para PySpark ---\n",
            "‚úÖ Tradu√ß√£o bem-sucedida!\n",
            "\n",
            "üîß --- C√≥digo PySpark Gerado ---\n",
            "df = spark.table('funcionarios').groupBy(F.col('cargo')).agg(F.avg(F.col('salario')).alias('salario_medio'))\n",
            "\n",
            "üéØ --- Aliases Detectados ---\n",
            "   GROUP ‚Üí funcionarios\n",
            "\n",
            "üìä --- Resultado PySpark ---\n",
            "+-------------+-------------+\n",
            "|        cargo|salario_medio|\n",
            "+-------------+-------------+\n",
            "|      Gerente|       9000.0|\n",
            "|Desenvolvedor|       6750.0|\n",
            "|     Analista|       4900.0|\n",
            "+-------------+-------------+\n",
            "\n",
            "üéØ --- Valida√ß√£o Final ---\n",
            "‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!\n",
            "‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas\n",
            "\n",
            "============================================================\n",
            "üéâ DEMONSTRA√á√ÉO COMPLETA!\n",
            "üí° O tradutor funciona corretamente para consultas simples e complexas\n",
            "üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas\n",
            "‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados\n"
          ]
        }
      ],
      "source": [
        "# üß™ Calcular o sal√°rio m√©dio por cargo:\n",
        "sql_complexo = '''\n",
        "SELECT cargo, AVG(salario) AS salario_medio\n",
        "FROM funcionarios\n",
        "GROUP BY cargo;\n",
        "'''\n",
        "\n",
        "print('üîç --- Consulta SQL Complexa ---')\n",
        "print(sql_complexo)\n",
        "print()\n",
        "\n",
        "# Executar com SparkSQL\n",
        "print('‚ö° --- Resultado SparkSQL ---')\n",
        "print(f'spark.slq(\"\"\"{sql_complexo}\"\"\")')\n",
        "df_sql_complexo = spark.sql(sql_complexo)\n",
        "df_sql_complexo.show()\n",
        "\n",
        "# Traduzir para PySpark\n",
        "print('üîÑ --- Traduzindo SQL Complexo para PySpark ---')\n",
        "translator = SimpleSQLTranslator()\n",
        "resultado_complexo = translator.translate(sql_complexo)\n",
        "\n",
        "if resultado_complexo['success']:\n",
        "    print('‚úÖ Tradu√ß√£o bem-sucedida!')\n",
        "    print()\n",
        "    print('üîß --- C√≥digo PySpark Gerado ---')\n",
        "    print(resultado_complexo['pyspark_code'])\n",
        "    print()\n",
        "\n",
        "    print('üéØ --- Aliases Detectados ---')\n",
        "    if resultado_complexo['table_aliases']:\n",
        "        for alias, real_name in resultado_complexo['table_aliases'].items():\n",
        "            print(f'   {alias} ‚Üí {real_name}')\n",
        "    else:\n",
        "        print('   Nenhum alias detectado')\n",
        "    print()\n",
        "\n",
        "    # Executar c√≥digo PySpark gerado\n",
        "    print('üìä --- Resultado PySpark ---')\n",
        "    try:\n",
        "        local_vars = {'spark': spark, 'F': F}\n",
        "        exec(resultado_complexo['pyspark_code'], globals(), local_vars)\n",
        "        df_pyspark_complexo = local_vars['df']\n",
        "        df_pyspark_complexo.show()\n",
        "\n",
        "        print('üéØ --- Valida√ß√£o Final ---')\n",
        "        validation = resultado_complexo['validation']\n",
        "        if validation['passed']:\n",
        "            print('‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!')\n",
        "            print('‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas')\n",
        "        else:\n",
        "            print(f'‚ùå Valida√ß√£o falhou: {validation[\"message\"]}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Erro ao executar c√≥digo PySpark: {e}')\n",
        "\n",
        "else:\n",
        "    print(f'‚ùå Erro na tradu√ß√£o: {resultado_complexo[\"error\"]}')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('üéâ DEMONSTRA√á√ÉO COMPLETA!')\n",
        "print('üí° O tradutor funciona corretamente para consultas simples e complexas')\n",
        "print('üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas')\n",
        "print('‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "i3qJcY7pX8kN",
        "outputId": "c9ea907c-4804-46c1-cdba-4a87a9bcff2a",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç --- Consulta SQL Complexa ---\n",
            "\n",
            "SELECT f.nome AS funcionario, p.nome AS projeto, a.horas_semanais\n",
            "FROM funcionarios f\n",
            "JOIN avaliacoes a ON f.id = a.funcionario_id\n",
            "JOIN projetos p ON a.projeto_id = p.id;\n",
            "\n",
            "\n",
            "‚ö° --- Resultado SparkSQL ---\n",
            "spark.slq(\"\"\"\n",
            "SELECT f.nome AS funcionario, p.nome AS projeto, a.horas_semanais\n",
            "FROM funcionarios f\n",
            "JOIN avaliacoes a ON f.id = a.funcionario_id\n",
            "JOIN projetos p ON a.projeto_id = p.id;\n",
            "\"\"\")\n",
            "+-----------+-------------+--------------+\n",
            "|funcionario|      projeto|horas_semanais|\n",
            "+-----------+-------------+--------------+\n",
            "|      Alice|Projeto Gamma|            20|\n",
            "|        Bob|Projeto Alpha|            30|\n",
            "|     Carlos|Projeto Gamma|            10|\n",
            "|      Diana|Projeto Alpha|            25|\n",
            "|        Eva| Projeto Beta|            15|\n",
            "+-----------+-------------+--------------+\n",
            "\n",
            "üîÑ --- Traduzindo SQL Complexo para PySpark ---\n",
            "‚úÖ Tradu√ß√£o bem-sucedida!\n",
            "\n",
            "üîß --- C√≥digo PySpark Gerado ---\n",
            "df = spark.table('funcionarios').join(spark.table('avaliacoes'), F.col('funcionarios.id') == F.col('avaliacoes.funcionario_id'), 'inner').join(spark.table('projetos'), F.col('avaliacoes.projeto_id') == F.col('projetos.id'), 'inner').select(F.col('funcionarios.nome').alias('funcionario'), F.col('projetos.nome').alias('projeto'), F.col('avaliacoes.horas_semanais'))\n",
            "\n",
            "üéØ --- Aliases Detectados ---\n",
            "   f ‚Üí funcionarios\n",
            "   a ‚Üí avaliacoes\n",
            "   p ‚Üí projetos\n",
            "\n",
            "üìä --- Resultado PySpark ---\n",
            "+-----------+-------------+--------------+\n",
            "|funcionario|      projeto|horas_semanais|\n",
            "+-----------+-------------+--------------+\n",
            "|      Alice|Projeto Gamma|            20|\n",
            "|        Bob|Projeto Alpha|            30|\n",
            "|     Carlos|Projeto Gamma|            10|\n",
            "|      Diana|Projeto Alpha|            25|\n",
            "|        Eva| Projeto Beta|            15|\n",
            "+-----------+-------------+--------------+\n",
            "\n",
            "üéØ --- Valida√ß√£o Final ---\n",
            "‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!\n",
            "‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas\n",
            "\n",
            "============================================================\n",
            "üéâ DEMONSTRA√á√ÉO COMPLETA!\n",
            "üí° O tradutor funciona corretamente para consultas simples e complexas\n",
            "üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas\n",
            "‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados\n"
          ]
        }
      ],
      "source": [
        "# üß™ Listar o nome do funcion√°rio e os projetos em que ele est√° envolvido, junto com as horas semanais:\n",
        "sql_complexo = '''\n",
        "SELECT f.nome AS funcionario, p.nome AS projeto, a.horas_semanais\n",
        "FROM funcionarios f\n",
        "JOIN avaliacoes a ON f.id = a.funcionario_id\n",
        "JOIN projetos p ON a.projeto_id = p.id;\n",
        "'''\n",
        "\n",
        "print('üîç --- Consulta SQL Complexa ---')\n",
        "print(sql_complexo)\n",
        "print()\n",
        "\n",
        "# Executar com SparkSQL\n",
        "print('‚ö° --- Resultado SparkSQL ---')\n",
        "print(f'spark.slq(\"\"\"{sql_complexo}\"\"\")')\n",
        "df_sql_complexo = spark.sql(sql_complexo)\n",
        "df_sql_complexo.show()\n",
        "\n",
        "# Traduzir para PySpark\n",
        "print('üîÑ --- Traduzindo SQL Complexo para PySpark ---')\n",
        "translator = SimpleSQLTranslator()\n",
        "resultado_complexo = translator.translate(sql_complexo)\n",
        "\n",
        "if resultado_complexo['success']:\n",
        "    print('‚úÖ Tradu√ß√£o bem-sucedida!')\n",
        "    print()\n",
        "    print('üîß --- C√≥digo PySpark Gerado ---')\n",
        "    print(resultado_complexo['pyspark_code'])\n",
        "    print()\n",
        "\n",
        "    print('üéØ --- Aliases Detectados ---')\n",
        "    if resultado_complexo['table_aliases']:\n",
        "        for alias, real_name in resultado_complexo['table_aliases'].items():\n",
        "            print(f'   {alias} ‚Üí {real_name}')\n",
        "    else:\n",
        "        print('   Nenhum alias detectado')\n",
        "    print()\n",
        "\n",
        "    # Executar c√≥digo PySpark gerado\n",
        "    print('üìä --- Resultado PySpark ---')\n",
        "    try:\n",
        "        local_vars = {'spark': spark, 'F': F}\n",
        "        exec(resultado_complexo['pyspark_code'], globals(), local_vars)\n",
        "        df_pyspark_complexo = local_vars['df']\n",
        "        df_pyspark_complexo.show()\n",
        "\n",
        "        print('üéØ --- Valida√ß√£o Final ---')\n",
        "        validation = resultado_complexo['validation']\n",
        "        if validation['passed']:\n",
        "            print('‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!')\n",
        "            print('‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas')\n",
        "        else:\n",
        "            print(f'‚ùå Valida√ß√£o falhou: {validation[\"message\"]}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Erro ao executar c√≥digo PySpark: {e}')\n",
        "\n",
        "else:\n",
        "    print(f'‚ùå Erro na tradu√ß√£o: {resultado_complexo[\"error\"]}')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('üéâ DEMONSTRA√á√ÉO COMPLETA!')\n",
        "print('üí° O tradutor funciona corretamente para consultas simples e complexas')\n",
        "print('üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas')\n",
        "print('‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MoGv-0KkYr6e",
        "outputId": "c3efec5f-4dea-4777-dc30-5c3e9fa2d01c",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç --- Consulta SQL Complexa ---\n",
            "\n",
            "SELECT nome, salario\n",
            "FROM funcionarios\n",
            "ORDER BY salario DESC\n",
            "LIMIT 1;\n",
            "\n",
            "\n",
            "‚ö° --- Resultado SparkSQL ---\n",
            "spark.slq(\"\"\"\n",
            "SELECT nome, salario\n",
            "FROM funcionarios\n",
            "ORDER BY salario DESC\n",
            "LIMIT 1;\n",
            "\"\"\")\n",
            "+------+-------+\n",
            "|  nome|salario|\n",
            "+------+-------+\n",
            "|Carlos|9000.00|\n",
            "+------+-------+\n",
            "\n",
            "üîÑ --- Traduzindo SQL Complexo para PySpark ---\n",
            "‚úÖ Tradu√ß√£o bem-sucedida!\n",
            "\n",
            "üîß --- C√≥digo PySpark Gerado ---\n",
            "df = spark.table('funcionarios').select(F.col('nome'), F.col('salario')).orderBy(F.col('salario').desc()).limit(1)\n",
            "\n",
            "üéØ --- Aliases Detectados ---\n",
            "   ORDER ‚Üí funcionarios\n",
            "\n",
            "üìä --- Resultado PySpark ---\n",
            "+------+-------+\n",
            "|  nome|salario|\n",
            "+------+-------+\n",
            "|Carlos|9000.00|\n",
            "+------+-------+\n",
            "\n",
            "üéØ --- Valida√ß√£o Final ---\n",
            "‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!\n",
            "‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas\n",
            "\n",
            "============================================================\n",
            "üéâ DEMONSTRA√á√ÉO COMPLETA!\n",
            "üí° O tradutor funciona corretamente para consultas simples e complexas\n",
            "üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas\n",
            "‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados\n"
          ]
        }
      ],
      "source": [
        "# üß™ Encontrar o funcion√°rio com o maior sal√°rio:\n",
        "sql_complexo = '''\n",
        "SELECT nome, salario\n",
        "FROM funcionarios\n",
        "ORDER BY salario DESC\n",
        "LIMIT 1;\n",
        "'''\n",
        "\n",
        "print('üîç --- Consulta SQL Complexa ---')\n",
        "print(sql_complexo)\n",
        "print()\n",
        "\n",
        "# Executar com SparkSQL\n",
        "print('‚ö° --- Resultado SparkSQL ---')\n",
        "print(f'spark.slq(\"\"\"{sql_complexo}\"\"\")')\n",
        "df_sql_complexo = spark.sql(sql_complexo)\n",
        "df_sql_complexo.show()\n",
        "\n",
        "# Traduzir para PySpark\n",
        "print('üîÑ --- Traduzindo SQL Complexo para PySpark ---')\n",
        "translator = SimpleSQLTranslator()\n",
        "resultado_complexo = translator.translate(sql_complexo)\n",
        "\n",
        "if resultado_complexo['success']:\n",
        "    print('‚úÖ Tradu√ß√£o bem-sucedida!')\n",
        "    print()\n",
        "    print('üîß --- C√≥digo PySpark Gerado ---')\n",
        "    print(resultado_complexo['pyspark_code'])\n",
        "    print()\n",
        "\n",
        "    print('üéØ --- Aliases Detectados ---')\n",
        "    if resultado_complexo['table_aliases']:\n",
        "        for alias, real_name in resultado_complexo['table_aliases'].items():\n",
        "            print(f'   {alias} ‚Üí {real_name}')\n",
        "    else:\n",
        "        print('   Nenhum alias detectado')\n",
        "    print()\n",
        "\n",
        "    # Executar c√≥digo PySpark gerado\n",
        "    print('üìä --- Resultado PySpark ---')\n",
        "    try:\n",
        "        local_vars = {'spark': spark, 'F': F}\n",
        "        exec(resultado_complexo['pyspark_code'], globals(), local_vars)\n",
        "        df_pyspark_complexo = local_vars['df']\n",
        "        df_pyspark_complexo.show()\n",
        "\n",
        "        print('üéØ --- Valida√ß√£o Final ---')\n",
        "        validation = resultado_complexo['validation']\n",
        "        if validation['passed']:\n",
        "            print('‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!')\n",
        "            print('‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas')\n",
        "        else:\n",
        "            print(f'‚ùå Valida√ß√£o falhou: {validation[\"message\"]}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Erro ao executar c√≥digo PySpark: {e}')\n",
        "\n",
        "else:\n",
        "    print(f'‚ùå Erro na tradu√ß√£o: {resultado_complexo[\"error\"]}')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('üéâ DEMONSTRA√á√ÉO COMPLETA!')\n",
        "print('üí° O tradutor funciona corretamente para consultas simples e complexas')\n",
        "print('üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas')\n",
        "print('‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SxycPoXRY2-u",
        "outputId": "57f15ba3-780c-4011-acbc-d00071709087",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç --- Consulta SQL Complexa ---\n",
            "\n",
            "SELECT d.nome AS departamento, COUNT(f.id) AS total_funcionarios\n",
            "FROM departamento d\n",
            "LEFT JOIN funcionarios f ON d.id = f.departamento_id\n",
            "GROUP BY d.nome;\n",
            "\n",
            "\n",
            "‚ö° --- Resultado SparkSQL ---\n",
            "spark.slq(\"\"\"\n",
            "SELECT d.nome AS departamento, COUNT(f.id) AS total_funcionarios\n",
            "FROM departamento d\n",
            "LEFT JOIN funcionarios f ON d.id = f.departamento_id\n",
            "GROUP BY d.nome;\n",
            "\"\"\")\n",
            "+----------------+------------------+\n",
            "|    departamento|total_funcionarios|\n",
            "+----------------+------------------+\n",
            "|Recursos Humanos|                 2|\n",
            "| Desenvolvimento|                 2|\n",
            "|       Marketing|                 1|\n",
            "+----------------+------------------+\n",
            "\n",
            "üîÑ --- Traduzindo SQL Complexo para PySpark ---\n",
            "‚úÖ Tradu√ß√£o bem-sucedida!\n",
            "\n",
            "üîß --- C√≥digo PySpark Gerado ---\n",
            "df = spark.table('departamento').join(spark.table('funcionarios'), F.col('departamento.id') == F.col('funcionarios.departamento_id'), 'left').groupBy(F.col('departamento.nome')).agg(F.count(F.col('funcionarios.id')).alias('total_funcionarios'))\n",
            "\n",
            "üéØ --- Aliases Detectados ---\n",
            "   d ‚Üí departamento\n",
            "   f ‚Üí funcionarios\n",
            "\n",
            "üìä --- Resultado PySpark ---\n",
            "+----------------+------------------+\n",
            "|            nome|total_funcionarios|\n",
            "+----------------+------------------+\n",
            "|Recursos Humanos|                 2|\n",
            "| Desenvolvimento|                 2|\n",
            "|       Marketing|                 1|\n",
            "+----------------+------------------+\n",
            "\n",
            "üéØ --- Valida√ß√£o Final ---\n",
            "‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!\n",
            "‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas\n",
            "\n",
            "============================================================\n",
            "üéâ DEMONSTRA√á√ÉO COMPLETA!\n",
            "üí° O tradutor funciona corretamente para consultas simples e complexas\n",
            "üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas\n",
            "‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados\n"
          ]
        }
      ],
      "source": [
        "# üß™ Contar quantos funcion√°rios tem por departamento:\n",
        "sql_complexo = '''\n",
        "SELECT d.nome AS departamento, COUNT(f.id) AS total_funcionarios\n",
        "FROM departamento d\n",
        "LEFT JOIN funcionarios f ON d.id = f.departamento_id\n",
        "GROUP BY d.nome;\n",
        "'''\n",
        "\n",
        "print('üîç --- Consulta SQL Complexa ---')\n",
        "print(sql_complexo)\n",
        "print()\n",
        "\n",
        "# Executar com SparkSQL\n",
        "print('‚ö° --- Resultado SparkSQL ---')\n",
        "print(f'spark.slq(\"\"\"{sql_complexo}\"\"\")')\n",
        "df_sql_complexo = spark.sql(sql_complexo)\n",
        "df_sql_complexo.show()\n",
        "\n",
        "# Traduzir para PySpark\n",
        "print('üîÑ --- Traduzindo SQL Complexo para PySpark ---')\n",
        "translator = SimpleSQLTranslator()\n",
        "resultado_complexo = translator.translate(sql_complexo)\n",
        "\n",
        "if resultado_complexo['success']:\n",
        "    print('‚úÖ Tradu√ß√£o bem-sucedida!')\n",
        "    print()\n",
        "    print('üîß --- C√≥digo PySpark Gerado ---')\n",
        "    print(resultado_complexo['pyspark_code'])\n",
        "    print()\n",
        "\n",
        "    print('üéØ --- Aliases Detectados ---')\n",
        "    if resultado_complexo['table_aliases']:\n",
        "        for alias, real_name in resultado_complexo['table_aliases'].items():\n",
        "            print(f'   {alias} ‚Üí {real_name}')\n",
        "    else:\n",
        "        print('   Nenhum alias detectado')\n",
        "    print()\n",
        "\n",
        "    # Executar c√≥digo PySpark gerado\n",
        "    print('üìä --- Resultado PySpark ---')\n",
        "    try:\n",
        "        local_vars = {'spark': spark, 'F': F}\n",
        "        exec(resultado_complexo['pyspark_code'], globals(), local_vars)\n",
        "        df_pyspark_complexo = local_vars['df']\n",
        "        df_pyspark_complexo.show()\n",
        "\n",
        "        print('üéØ --- Valida√ß√£o Final ---')\n",
        "        validation = resultado_complexo['validation']\n",
        "        if validation['passed']:\n",
        "            print('‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!')\n",
        "            print('‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas')\n",
        "        else:\n",
        "            print(f'‚ùå Valida√ß√£o falhou: {validation[\"message\"]}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Erro ao executar c√≥digo PySpark: {e}')\n",
        "\n",
        "else:\n",
        "    print(f'‚ùå Erro na tradu√ß√£o: {resultado_complexo[\"error\"]}')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('üéâ DEMONSTRA√á√ÉO COMPLETA!')\n",
        "print('üí° O tradutor funciona corretamente para consultas simples e complexas')\n",
        "print('üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas')\n",
        "print('‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bga3kh92Y-JI",
        "outputId": "1f4107d0-e881-4641-dbae-329c96eacf0e",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç --- Consulta SQL Complexa ---\n",
            "\n",
            "SELECT f.nome AS funcionario, a.horas_semanais\n",
            "FROM funcionarios f\n",
            "JOIN avaliacoes a ON f.id = a.funcionario_id\n",
            "JOIN projetos p ON a.projeto_id = p.id\n",
            "WHERE p.nome = 'Projeto Alpha';\n",
            "\n",
            "\n",
            "‚ö° --- Resultado SparkSQL ---\n",
            "spark.slq(\"\"\"\n",
            "SELECT f.nome AS funcionario, a.horas_semanais\n",
            "FROM funcionarios f\n",
            "JOIN avaliacoes a ON f.id = a.funcionario_id\n",
            "JOIN projetos p ON a.projeto_id = p.id\n",
            "WHERE p.nome = 'Projeto Alpha';\n",
            "\"\"\")\n",
            "+-----------+--------------+\n",
            "|funcionario|horas_semanais|\n",
            "+-----------+--------------+\n",
            "|        Bob|            30|\n",
            "|      Diana|            25|\n",
            "+-----------+--------------+\n",
            "\n",
            "üîÑ --- Traduzindo SQL Complexo para PySpark ---\n",
            "‚úÖ Tradu√ß√£o bem-sucedida!\n",
            "\n",
            "üîß --- C√≥digo PySpark Gerado ---\n",
            "df = spark.table('funcionarios').join(spark.table('avaliacoes'), F.col('funcionarios.id') == F.col('avaliacoes.funcionario_id'), 'inner').join(spark.table('projetos'), F.col('avaliacoes.projeto_id') == F.col('projetos.id'), 'inner').filter(F.col('projetos.nome') == F.lit('Projeto Alpha')).select(F.col('funcionarios.nome').alias('funcionario'), F.col('avaliacoes.horas_semanais'))\n",
            "\n",
            "üéØ --- Aliases Detectados ---\n",
            "   f ‚Üí funcionarios\n",
            "   a ‚Üí avaliacoes\n",
            "   p ‚Üí projetos\n",
            "\n",
            "üìä --- Resultado PySpark ---\n",
            "+-----------+--------------+\n",
            "|funcionario|horas_semanais|\n",
            "+-----------+--------------+\n",
            "|        Bob|            30|\n",
            "|      Diana|            25|\n",
            "+-----------+--------------+\n",
            "\n",
            "üéØ --- Valida√ß√£o Final ---\n",
            "‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!\n",
            "‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas\n",
            "\n",
            "============================================================\n",
            "üéâ DEMONSTRA√á√ÉO COMPLETA!\n",
            "üí° O tradutor funciona corretamente para consultas simples e complexas\n",
            "üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas\n",
            "‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados\n"
          ]
        }
      ],
      "source": [
        "# üß™ Listar o nome dos funcion√°rios que trabalham no \"Projeto Alpha\" e suas horas semanais:\n",
        "sql_complexo = '''\n",
        "SELECT f.nome AS funcionario, a.horas_semanais\n",
        "FROM funcionarios f\n",
        "JOIN avaliacoes a ON f.id = a.funcionario_id\n",
        "JOIN projetos p ON a.projeto_id = p.id\n",
        "WHERE p.nome = 'Projeto Alpha';\n",
        "'''\n",
        "\n",
        "print('üîç --- Consulta SQL Complexa ---')\n",
        "print(sql_complexo)\n",
        "print()\n",
        "\n",
        "# Executar com SparkSQL\n",
        "print('‚ö° --- Resultado SparkSQL ---')\n",
        "print(f'spark.slq(\"\"\"{sql_complexo}\"\"\")')\n",
        "df_sql_complexo = spark.sql(sql_complexo)\n",
        "df_sql_complexo.show()\n",
        "\n",
        "# Traduzir para PySpark\n",
        "print('üîÑ --- Traduzindo SQL Complexo para PySpark ---')\n",
        "translator = SimpleSQLTranslator()\n",
        "resultado_complexo = translator.translate(sql_complexo)\n",
        "\n",
        "if resultado_complexo['success']:\n",
        "    print('‚úÖ Tradu√ß√£o bem-sucedida!')\n",
        "    print()\n",
        "    print('üîß --- C√≥digo PySpark Gerado ---')\n",
        "    print(resultado_complexo['pyspark_code'])\n",
        "    print()\n",
        "\n",
        "    print('üéØ --- Aliases Detectados ---')\n",
        "    if resultado_complexo['table_aliases']:\n",
        "        for alias, real_name in resultado_complexo['table_aliases'].items():\n",
        "            print(f'   {alias} ‚Üí {real_name}')\n",
        "    else:\n",
        "        print('   Nenhum alias detectado')\n",
        "    print()\n",
        "\n",
        "    # Executar c√≥digo PySpark gerado\n",
        "    print('üìä --- Resultado PySpark ---')\n",
        "    try:\n",
        "        local_vars = {'spark': spark, 'F': F}\n",
        "        exec(resultado_complexo['pyspark_code'], globals(), local_vars)\n",
        "        df_pyspark_complexo = local_vars['df']\n",
        "        df_pyspark_complexo.show()\n",
        "\n",
        "        print('üéØ --- Valida√ß√£o Final ---')\n",
        "        validation = resultado_complexo['validation']\n",
        "        if validation['passed']:\n",
        "            print('‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!')\n",
        "            print('‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas')\n",
        "        else:\n",
        "            print(f'‚ùå Valida√ß√£o falhou: {validation[\"message\"]}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Erro ao executar c√≥digo PySpark: {e}')\n",
        "\n",
        "else:\n",
        "    print(f'‚ùå Erro na tradu√ß√£o: {resultado_complexo[\"error\"]}')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('üéâ DEMONSTRA√á√ÉO COMPLETA!')\n",
        "print('üí° O tradutor funciona corretamente para consultas simples e complexas')\n",
        "print('üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas')\n",
        "print('‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ Consulta SQL com COALESCE\n",
        "sql_complexo = '''\n",
        "SELECT\n",
        "    f.nome AS nome_funcionario,\n",
        "    COALESCE(p.nome, 'N√£o Alocado em Projeto') AS nome_do_projeto\n",
        "FROM\n",
        "    funcionarios f\n",
        "LEFT JOIN\n",
        "    avaliacoes a ON f.id = a.funcionario_id\n",
        "LEFT JOIN\n",
        "    projetos p ON a.projeto_id = p.id;\n",
        "'''\n",
        "\n",
        "print('üîç --- Consulta SQL Complexa ---')\n",
        "print(sql_complexo)\n",
        "print()\n",
        "\n",
        "# Executar com SparkSQL\n",
        "print('‚ö° --- Resultado SparkSQL ---')\n",
        "print(f'spark.slq(\"\"\"{sql_complexo}\"\"\")')\n",
        "df_sql_complexo = spark.sql(sql_complexo)\n",
        "df_sql_complexo.show()\n",
        "\n",
        "# Traduzir para PySpark\n",
        "print('üîÑ --- Traduzindo SQL Complexo para PySpark ---')\n",
        "translator = SimpleSQLTranslator()\n",
        "resultado_complexo = translator.translate(sql_complexo)\n",
        "\n",
        "if resultado_complexo['success']:\n",
        "    print('‚úÖ Tradu√ß√£o bem-sucedida!')\n",
        "    print()\n",
        "    print('üîß --- C√≥digo PySpark Gerado ---')\n",
        "    print(resultado_complexo['pyspark_code'])\n",
        "    print()\n",
        "\n",
        "    print('üéØ --- Aliases Detectados ---')\n",
        "    if resultado_complexo['table_aliases']:\n",
        "        for alias, real_name in resultado_complexo['table_aliases'].items():\n",
        "            print(f'   {alias} ‚Üí {real_name}')\n",
        "    else:\n",
        "        print('   Nenhum alias detectado')\n",
        "    print()\n",
        "\n",
        "    # Executar c√≥digo PySpark gerado\n",
        "    print('üìä --- Resultado PySpark ---')\n",
        "    try:\n",
        "        local_vars = {'spark': spark, 'F': F}\n",
        "        exec(resultado_complexo['pyspark_code'], globals(), local_vars)\n",
        "        df_pyspark_complexo = local_vars['df']\n",
        "        df_pyspark_complexo.show()\n",
        "\n",
        "        print('üéØ --- Valida√ß√£o Final ---')\n",
        "        validation = resultado_complexo['validation']\n",
        "        if validation['passed']:\n",
        "            print('‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!')\n",
        "            print('‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas')\n",
        "        else:\n",
        "            print(f'‚ùå Valida√ß√£o falhou: {validation[\"message\"]}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Erro ao executar c√≥digo PySpark: {e}')\n",
        "\n",
        "else:\n",
        "    print(f'‚ùå Erro na tradu√ß√£o: {resultado_complexo[\"error\"]}')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('üéâ DEMONSTRA√á√ÉO COMPLETA!')\n",
        "print('üí° O tradutor funciona corretamente para consultas simples e complexas')\n",
        "print('üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas')\n",
        "print('‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QrTisHaBgLnz",
        "outputId": "927c10c9-ef0e-460f-e4e4-1484fc8dfd4c"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç --- Consulta SQL Complexa ---\n",
            "\n",
            "SELECT\n",
            "    f.nome AS nome_funcionario,\n",
            "    COALESCE(p.nome, 'N√£o Alocado em Projeto') AS nome_do_projeto\n",
            "FROM\n",
            "    funcionarios f\n",
            "LEFT JOIN\n",
            "    avaliacoes a ON f.id = a.funcionario_id\n",
            "LEFT JOIN\n",
            "    projetos p ON a.projeto_id = p.id;\n",
            "\n",
            "\n",
            "‚ö° --- Resultado SparkSQL ---\n",
            "spark.slq(\"\"\"\n",
            "SELECT\n",
            "    f.nome AS nome_funcionario,\n",
            "    COALESCE(p.nome, 'N√£o Alocado em Projeto') AS nome_do_projeto\n",
            "FROM\n",
            "    funcionarios f\n",
            "LEFT JOIN\n",
            "    avaliacoes a ON f.id = a.funcionario_id\n",
            "LEFT JOIN\n",
            "    projetos p ON a.projeto_id = p.id;\n",
            "\"\"\")\n",
            "+----------------+---------------+\n",
            "|nome_funcionario|nome_do_projeto|\n",
            "+----------------+---------------+\n",
            "|           Alice|  Projeto Gamma|\n",
            "|             Bob|  Projeto Alpha|\n",
            "|          Carlos|  Projeto Gamma|\n",
            "|           Diana|  Projeto Alpha|\n",
            "|             Eva|   Projeto Beta|\n",
            "+----------------+---------------+\n",
            "\n",
            "üîÑ --- Traduzindo SQL Complexo para PySpark ---\n",
            "‚úÖ Tradu√ß√£o bem-sucedida!\n",
            "\n",
            "üîß --- C√≥digo PySpark Gerado ---\n",
            "df = spark.table('funcionarios').join(spark.table('avaliacoes'), F.col('funcionarios.id') == F.col('avaliacoes.funcionario_id'), 'left').join(spark.table('projetos'), F.col('avaliacoes.projeto_id') == F.col('projetos.id'), 'left').select(F.col('funcionarios.nome').alias('nome_funcionario'), F.coalesce(F.col('projetos.nome'), F.lit('N√£o Alocado em Projeto')).alias('nome_do_projeto'))\n",
            "\n",
            "üéØ --- Aliases Detectados ---\n",
            "   f ‚Üí funcionarios\n",
            "   a ‚Üí avaliacoes\n",
            "   p ‚Üí projetos\n",
            "\n",
            "üìä --- Resultado PySpark ---\n",
            "+----------------+---------------+\n",
            "|nome_funcionario|nome_do_projeto|\n",
            "+----------------+---------------+\n",
            "|           Alice|  Projeto Gamma|\n",
            "|             Bob|  Projeto Alpha|\n",
            "|          Carlos|  Projeto Gamma|\n",
            "|           Diana|  Projeto Alpha|\n",
            "|             Eva|   Projeto Beta|\n",
            "+----------------+---------------+\n",
            "\n",
            "üéØ --- Valida√ß√£o Final ---\n",
            "‚úÖ Valida√ß√£o passou: Todos os aliases foram resolvidos corretamente!\n",
            "‚úÖ C√≥digo PySpark usa apenas nomes reais de tabelas\n",
            "\n",
            "============================================================\n",
            "üéâ DEMONSTRA√á√ÉO COMPLETA!\n",
            "üí° O tradutor funciona corretamente para consultas simples e complexas\n",
            "üéØ Aliases s√£o sempre resolvidos para nomes reais de tabelas\n",
            "‚ö° C√≥digo PySpark gerado √© execut√°vel e produz os mesmos resultados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2111fb39"
      },
      "source": [
        "## üéâ Conclus√£o\n",
        "\n",
        "### ‚úÖ Tradutor Funcional\n",
        "\n",
        "Este notebook cont√©m uma **solu√ß√£o limpa e funcional** para tradu√ß√£o SQL ‚Üí PySpark:\n",
        "\n",
        "**üîß Componentes:**\n",
        "- `SQLParser`: Parser robusto para an√°lise SQL\n",
        "- `SimpleSQLTranslator`: Tradutor com resolu√ß√£o de aliases\n",
        "- Testes automatizados e valida√ß√£o\n",
        "\n",
        "**‚ú® Funcionalidades:**\n",
        "- ‚úÖ SELECT, FROM, JOIN, WHERE, ORDER BY\n",
        "- ‚úÖ Resolu√ß√£o correta de aliases (sempre usa nomes reais)\n",
        "- ‚úÖ Suporte a COALESCE\n",
        "- ‚úÖ Valida√ß√£o autom√°tica\n",
        "- ‚úÖ C√≥digo PySpark execut√°vel\n",
        "\n",
        "**üéØ Validado:**\n",
        "- Nenhum alias aparece no c√≥digo PySpark final\n",
        "- Apenas nomes reais de tabelas s√£o utilizados\n",
        "- Evita AnalysisException do Spark\n",
        "\n",
        "### üí° Como usar:\n",
        "\n",
        "```python\n",
        "# Criar tradutor\n",
        "translator = SimpleSQLTranslator()\n",
        "\n",
        "# Traduzir SQL\n",
        "result = translator.translate(\"SELECT f.nome FROM funcionarios f\")\n",
        "\n",
        "# Obter c√≥digo PySpark\n",
        "print(result['pyspark_code'])\n",
        "```\n",
        "\n",
        "**üöÄ Pronto para uso em produ√ß√£o!**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}