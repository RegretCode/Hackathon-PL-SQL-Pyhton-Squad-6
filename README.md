# Hackathon-PL-SQL-Pyhton-Squad-6

Ferramenta para transformar SQL legado (Oracle, PostgreSQL) em PySpark e SparkSQL.

## Funcionalidades
- Convers√£o autom√°tica SQL ‚Üí PySpark/SparkSQL
- Detec√ß√£o de dialetos Oracle/PostgreSQL
- Interface web Streamlit
- CLI para processamento em lote
- Suporte a JOINs, GROUP BY, fun√ß√µes agregadas

## Convers√µes de Dialetos
**Oracle ‚Üí Spark:**
- ROWNUM ‚Üí ROW_NUMBER()
- SYSDATE ‚Üí CURRENT_TIMESTAMP
- NVL ‚Üí COALESCE
- DECODE ‚Üí CASE WHEN

**PostgreSQL ‚Üí Spark:**
- ILIKE ‚Üí LIKE
- NOW() ‚Üí CURRENT_TIMESTAMP
- EXTRACT ‚Üí DATE_PART

## Instala√ß√£o
```bash
pip install -r requirements.txt
```

## Uso

### Interface Web
```bash
streamlit run interface/app.py
```

### CLI
```bash
python -m sql_transformer.cli -f query.sql -o both
```

### Program√°tico
```python
from sql_transformer.parser import parse_sql
from sql_transformer.converter_pyspark import convert_to_pyspark

sql = "SELECT nome FROM clientes WHERE idade > 30"
parsed = parse_sql(sql)
result = convert_to_pyspark(parsed)
```

## üìä Exemplos de Convers√£o

### Exemplo 1: Query Simples
**SQL Original:**
```sql
SELECT nome, idade FROM clientes WHERE idade > 30
```

**PySpark Gerado:**
```python
df = spark.table('clientes').filter(F.col('idade') > F.lit(30)).select(F.col('nome'), F.col('idade'))
```

**SparkSQL Gerado:**
```python
spark.sql("""
SELECT nome, idade FROM clientes WHERE idade > 30
""")
```

### Exemplo 2: Query com JOIN e Agrega√ß√£o
**SQL Original:**
```sql
SELECT c.categoria, COUNT(*) as total, AVG(c.preco) as preco_medio
FROM produtos c
INNER JOIN vendas v ON c.id = v.produto_id
WHERE v.data_venda >= '2024-01-01'
GROUP BY c.categoria
HAVING COUNT(*) > 10
ORDER BY preco_medio DESC
```

**PySpark Gerado:**
```python
df = spark.table('produtos').join(spark.table('vendas'), F.col('produtos.id') == F.col('vendas.produto_id'), 'inner').filter(F.col('vendas.data_venda >') == F.lit('2024-01-01')).select(F.col('produtos.categoria'), F.count(F.col('*')).alias('total'), F.avg(F.col('produtos.preco')).alias('preco_medio')).orderBy(F.col('preco_medio').desc())
```

### Exemplo 3: Convers√£o Oracle
**SQL Oracle Original:**
```sql
SELECT nome, salario, ROWNUM
FROM (
    SELECT nome, salario
    FROM funcionarios
    WHERE NVL(ativo, 'N') = 'S'
    ORDER BY salario DESC
)
WHERE ROWNUM <= 5
```

**SQL Convertido para Spark:**
```sql
SELECT nome, salario, ROW_NUMBER() OVER (ORDER BY 1)
FROM (
    SELECT nome, salario
    FROM funcionarios
    WHERE COALESCE(ativo, 'N') = 'S'
    ORDER BY salario DESC
)
WHERE ROW_NUMBER() OVER (ORDER BY 1) <= 5
```

## üèóÔ∏è Arquitetura

```
sql_transformer/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ main.py                 # Demonstra√ß√µes
‚îú‚îÄ‚îÄ parser.py              # Parser SQL avan√ßado
‚îú‚îÄ‚îÄ converter_pyspark.py   # Conversor para PySpark
‚îú‚îÄ‚îÄ converter_sparksql.py  # Conversor para SparkSQL
‚îú‚îÄ‚îÄ dialect_converter.py  # Convers√£o de dialetos
‚îî‚îÄ‚îÄ cli.py                # Interface de linha de comando

interface/
‚îî‚îÄ‚îÄ app.py                # Interface web Streamlit

requirements.txt          # Depend√™ncias
README.md                # Documenta√ß√£o
```

### Componentes Principais

1. **Parser SQL** (`parser.py`)
   - An√°lise sint√°tica avan√ßada
   - Suporte a constru√ß√µes complexas
   - Extra√ß√£o de metadados estruturados

2. **Conversores** (`converter_*.py`)
   - Gera√ß√£o de c√≥digo PySpark otimizado
   - Formata√ß√£o de SparkSQL limpo
   - Tratamento de casos especiais

3. **Conversor de Dialetos** (`dialect_converter.py`)
   - Detec√ß√£o autom√°tica de dialetos
   - Transforma√ß√µes espec√≠ficas Oracle/PostgreSQL
   - Compatibilidade com Spark SQL

4. **Interface Web** (`interface/app.py`)
   - Interface intuitiva com Streamlit
   - Valida√ß√£o em tempo real
   - Download de arquivos gerados

5. **CLI** (`cli.py`)
   - Processamento em lote
   - Automa√ß√£o corporativa
   - Relat√≥rios de convers√£o

## üéØ Casos de Uso

### 1. Migra√ß√£o de Data Warehouse
- Convers√£o de procedures Oracle para Spark jobs
- Moderniza√ß√£o de ETL legado
- Migra√ß√£o para arquiteturas cloud-native

### 2. Desenvolvimento √Ågil
- Prototipagem r√°pida de queries Spark
- Convers√£o de consultas ad-hoc
- Padroniza√ß√£o de c√≥digo

### 3. Treinamento e Educa√ß√£o
- Aprendizado de PySpark atrav√©s de SQL familiar
- Compara√ß√£o entre dialetos
- Demonstra√ß√µes pr√°ticas

### 4. Automa√ß√£o Corporativa
- Processamento em lote de scripts legados
- Integra√ß√£o em pipelines CI/CD
- Relat√≥rios de migra√ß√£o

## üîß Configura√ß√£o Avan√ßada

### Vari√°veis de Ambiente
```bash
# Configurar Spark (opcional)
export SPARK_HOME=/path/to/spark
export PYSPARK_PYTHON=python3

# Configurar logging
export SQL_TRANSFORMER_LOG_LEVEL=INFO
```

### Personaliza√ß√£o
O sistema pode ser estendido atrav√©s de:
- Novos conversores de dialetos
- Fun√ß√µes SQL customizadas
- Templates de c√≥digo personalizados

## ü§ù Contribui√ß√£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudan√ßas (`git commit -am 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

## üìù Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

## üÜò Suporte

- **Issues**: Reporte bugs e solicite features no GitHub
- **Documenta√ß√£o**: Consulte este README e os coment√°rios no c√≥digo
- **Exemplos**: Execute `python sql_transformer/main.py` para ver demonstra√ß√µes

## üöÄ Roadmap

### Pr√≥ximas Vers√µes
- [ ] Suporte a mais dialetos (SQL Server, MySQL)
- [ ] Otimiza√ß√µes de performance autom√°ticas
- [ ] Integra√ß√£o com Databricks
- [ ] Suporte a Delta Lake
- [ ] API REST para integra√ß√£o
- [ ] Plugin para IDEs populares

---

**Desenvolvido para facilitar a moderniza√ß√£o de sistemas legados e acelerar a ado√ß√£o do Apache Spark em ambientes corporativos.**